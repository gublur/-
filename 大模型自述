                                              大模型自述

1.大模型语言基础

       定义：通常指的是具有超大规模参数的预训练语言模型
			 
       架构：主要以transforme的解码器架构 
			 
       训练model：预训练（base model),后训练（instruct model）
			 
    预训练（Pre-traingning)                           后训练(Post-traninging)                                       下游应用

    数据：海量文本数据 （数万亿的词元自然语言文本）      数据：大量指令数据（数十万，数百万千万指数数据）                   测试

    优化：预测下一个词   base model--->                优化：SFT（监督微调）,RL（强化学习）等方法  instruct model --->   推理

    目标：建立模型的基础能力                            目标：增强模型的任务能力
    使用方式：few-shot提示                             使用方式：zero-shot使用
		
2.预训练（Pre-traing)

  预训练（Pre_traing):
	
	>使用与下游任务无关的大规模数据进行模型参数的初始训练          
	>基于transformer解码器架构，对下一个词预测
	>数据数量，数据质量非常关键
	
3.后训练（Post-traing)

    后训练（post_traing):
	
	 >指令微调(Iinstruction Tuning),使用输入与输出配对的指令数据进行对模型微调.提升模型对问答形式的任务求解能力.
	 >人类对齐（Human AIignment),将大语言模型与人类的期望，需求及价值观对齐.基于人类反馈的强化学习对齐方法（RLHF)
  
